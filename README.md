# IMU Motion Recognition using Sensor Fusion on Raspberry Pi

This project implements a real-time gesture recognition system using the **Sense HAT** on a **Raspberry Pi**, leveraging 6-axis IMU data (accelerometer + gyroscope) to classify gestures like `move_none`, `move_circle`, `move_shake`, and `move_twist`. The trained model is deployed using **TensorFlow Lite** and provides visual output on the Sense HAT LED matrix.

---

## ğŸ“Œ Features

- Real-time gesture recognition on Raspberry Pi  
- IMU data fusion using accelerometer and gyroscope  
- Model training using TensorFlow  
- Deployment via TensorFlow Lite  
- LED output mapped to gestures using color codes  
- Graphical plots for 6-axis IMU signals  

---

## ğŸ“ Repository Structure

```
IMU_motion_recognition/
â”œâ”€â”€ data/                       # Raw .npy gesture recordings
â”œâ”€â”€ graphs/                     # 6-axis IMU signal graph images
â”œâ”€â”€ images/                     # Photos of LED matrix & hardware
â”œâ”€â”€ model/                      # Trained model (.tflite)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_capture.py         # Script to collect IMU gesture data
â”‚   â”œâ”€â”€ train_model.py          # Model training script
â”‚   â”œâ”€â”€ predict.py              # Real-time inference script
â”œâ”€â”€ report/
â”‚   â””â”€â”€ imu_gesture_report.tex  # Final LaTeX project report
â”œâ”€â”€ README.md                   # Project description (this file)
```

---

## ğŸ§  Model Overview

- **Input shape**: 50 time steps Ã— 6 features (flattened to 300)
- **Architecture**: Fully Connected Neural Network with 2 hidden layers
- **Output**: Softmax over 4 gesture classes
- **Framework**: TensorFlow â†’ TensorFlow Lite

---

## ğŸ¯ Gestures & LED Color Mapping

| Gesture       | LED Matrix Color |
|---------------|------------------|
| move_none     | Black (OFF)      |
| move_circle   | Red              |
| move_shake    | Green            |
| move_twist    | Blue             |

---

## ğŸš€ Getting Started

### âœ… Requirements

- Raspberry Pi 4 or 5  
- Sense HAT  
- Python 3.9+  
- TensorFlow & TensorFlow Lite  
- NumPy, scikit-learn  

### ğŸ”§ Setup Instructions

```bash
git clone https://github.com/LoKI-Codestar/IMU_motion_recognition.git
cd IMU_motion_recognition
```

1. Collect gesture data using `src/data_capture.py`  
2. Train your model using `src/train_model.py`  
3. Deploy and run live inference using `src/predict.py`

---

## ğŸ“Š Sensor Output Graphs

The project includes time-series plots for each gesture showing 6-axis IMU signals (x, y, z from accelerometer and gyroscope). These are available under the `graphs/` folder.

---

## ğŸ“¸ Hardware Setup

Photos of the Raspberry Pi + Sense HAT setup, along with LED output during prediction, are available under the `images/` directory.

---

## ğŸ“„ Lab Report

The complete technical documentation is written using LaTeX and can be found at:

```
report/imu_gesture_report.tex
```

This includes sections like Introduction, Methodology, Model Training, Results, Challenges, and Conclusion.

---

## ğŸ™‹â€â™‚ï¸ Author

**Sahil Gore**  
TH Deggendorf  
[GitHub: LoKI-Codestar](https://github.com/LoKI-Codestar)

---

## ğŸ“š References

- [TensorFlow Lite Documentation](https://www.tensorflow.org/lite)  
- [Sense HAT Python API](https://pythonhosted.org/sense-hat/)  
- [Raspberry Pi Documentation](https://www.raspberrypi.com/documentation/)  
- THD Lab05 â€“ Prof. Tobias Schaffer

---

## ğŸ™Œ Contributions

Feel free to fork the repo, suggest improvements, or open issues. PRs are welcome!