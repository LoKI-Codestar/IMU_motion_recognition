# IMU-Based Gesture Recognition using Raspberry Pi and TensorFlow Lite

This project implements real-time gesture recognition on a Raspberry Pi using IMU (accelerometer + gyroscope) sensor fusion via the Sense HAT module. A trained neural network classifies motion gestures and displays the result using color-coded output on the LED matrix.

---

## ğŸ”§ Project Overview

- **Platform**: Raspberry Pi 4 Model B  
- **Sensors**: Sense HAT (Accelerometer + Gyroscope)  
- **Framework**: TensorFlow & TensorFlow Lite  
- **Language**: Python 3  
- **Recognition**: 4 predefined gestures:
  - `move_none`
  - `move_circle`
  - `move_shake`
  - `move_twist`

---

## ğŸ§  Model Pipeline

1. **Data Collection**  
   Raw 6-axis IMU data recorded at 50 Hz for 1 second (50 samples).

2. **Data Processing**  
   Each sample is flattened into a vector and labeled according to the gesture performed.

3. **Model Training**  
   A simple fully connected neural network is trained using TensorFlow and exported to `.tflite`.

4. **Deployment**  
   Real-time inference is performed on the Pi using TensorFlow Lite, and results are shown on the LED matrix.

---

## ğŸ“ Repository Structure

```
â”œâ”€â”€ motion_data/              # Contains labeled .npy files for each gesture
â”‚   â”œâ”€â”€ move_circle/
â”‚   â”œâ”€â”€ move_none/
â”‚   â”œâ”€â”€ move_shake/
â”‚   â””â”€â”€ move_twist/
â”‚
â”œâ”€â”€ data_capture.py           # Script to record gesture samples
â”œâ”€â”€ train_model.py            # (optional) Script to train and export the model
â”œâ”€â”€ predict.py                # Real-time inference script for Raspberry Pi
â”œâ”€â”€ motion_model.tflite       # Trained and exported model
â”œâ”€â”€ plots/                    # IMU signal graphs for each gesture
â”‚
â”œâ”€â”€ report/                   # LaTeX project report
â”‚   â””â”€â”€ imu_gesture_report.tex
â”‚
â””â”€â”€ README.md                 # You're reading this!
```

---

## ğŸš€ Setup Instructions

1. **Clone the repository**
   ```bash
   git clone https://github.com/LoKI-Codestar/<your-repo-name>.git
   cd <your-repo-name>
   ```

2. **Install Dependencies**
   ```bash
   sudo apt update
   pip install numpy tensorflow scikit-learn sense-hat
   ```

3. **Collect Data**
   Run `data_capture.py` and perform gestures when prompted.  
   Modify the `LABEL` variable as needed.

4. **Run Inference**
   Deploy `motion_model.tflite` to Raspberry Pi and run `predict.py`.

---

## ğŸ“Š Sample Results

- Accuracy: ~95% on training, ~90% on validation
- Inference time: 20â€“40 ms
- LED Output:
  - `move_none` â€“ Black (off)
  - `move_circle` â€“ Red
  - `move_shake` â€“ Green
  - `move_twist` â€“ Blue

---

## ğŸ“¸ Screenshots

<p float="left">
  <img src="plots/move_circle_example1.png" width="45%" />
  <img src="plots/move_circle_example2.png" width="45%" />
</p>

<p float="left">
  <img src="plots/move_twist_example1.png" width="45%" />
  <img src="plots/move_twist_example2.png" width="45%" />
</p>

---

## ğŸ“„ Report

The complete lab report with methodology, graphs, and analysis is available in the `report/` folder.  
ğŸ“„ [`imu_gesture_report.tex`](report/imu_gesture_report.tex)

---

## ğŸ“š References

- [TensorFlow Lite](https://www.tensorflow.org/lite)
- [Sense HAT Python API](https://pythonhosted.org/sense-hat/)
- [Raspberry Pi Docs](https://www.raspberrypi.com/documentation/)

---

## ğŸ‘¤ Author

**Sahil Gore**  
TH Deggendorf â€“ Embedded Systems Lab 05  
Instructor: Prof. Tobias Schaffer

---